{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e77cb13d",
   "metadata": {},
   "source": [
    "# Natural Language Processing Project\n",
    "## NLP Course @ Politecnico di Milano 2024/2025 - Prof. Mark Carman\n",
    "### Topic 8: Medical Question Answering\n",
    "Dataset: \n",
    "* PubMedQA [link](https://huggingface.co/datasets/qiaojin/PubMedQA )\n",
    "\n",
    "Reference paper:\n",
    "* PubMedQA: A Dataset for Biomedical Research Question Answering [Link](https://arxiv.org/pdf/1909.06146)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98da54ef",
   "metadata": {},
   "source": [
    "## Group members:\n",
    "\n",
    "* Ketrin Mehmeti\n",
    "* Giulia Ghiazza\n",
    "* Leonardo Giorgio Franco\n",
    "* Edoardo Franco Mattei\n",
    "* Alessandro Epifania"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3ac59c",
   "metadata": {},
   "source": [
    "## Introduction\n",
    " \n",
    "The PubMedQA dataset is an innovative resource for question answering (QA) in the biomedical field, created from abstracts of scientific articles available on PubMed. The main purpose of PubMedQA is to assess the reasoning and inference abilities of intelligent systems on natural language, particularly within the context of biomedical research texts, which often require the processing of quantitative content.\n",
    "\n",
    "A typical instance in PubMedQA consists of the following components:\n",
    "\n",
    "* A question, which can either be the original title of a research paper or derived from it. For example: \"Do preoperative statins reduce atrial fibrillation after coronary artery bypass grafting?\"\n",
    "\n",
    "* A context, which is the abstract corresponding to the question, excluding its conclusion.\n",
    "\n",
    "* A long answer, represented by the conclusion of the abstract, which is expected to answer the research question.\n",
    "\n",
    "* A short answer in the form of \"yes,\" \"no,\" or \"maybe,\" summarizing the conclusion. In the provided example, the long answer is: \"(Conclusion) Our study indicated that preoperative statin therapy seems to reduce AF development after CABG,\" while the short answer is \"yes.\"\n",
    "\n",
    "The PubMedQA dataset is divided into three subsets:\n",
    "\n",
    "* PQA-L (Labeled): Contains 1k manually annotated instances with yes/no/maybe answers. These annotations were made in two modes: \"reasoning-free,\" where the annotator had access to the long answer, and \"reasoning-required,\" where the annotator could only rely on the context.\n",
    "\n",
    "* PQA-U (Unlabeled): Consists of 61.2k unlabeled instances, made up of PubMed articles with question-form titles and structured abstracts.\n",
    "\n",
    "* PQA-A (Artificial): Includes 211.3k artificially generated instances, where article titles in statement form are converted into questions, and yes/no answers are automatically assigned based on the presence or absence of negations in the original title.\n",
    "\n",
    "A key feature of PubMedQA is that the contexts are generated to directly answer the questions, with both components written by the same authors, ensuring a strong relationship between the question and context. This makes PubMedQA an ideal benchmark for testing the scientific reasoning capabilities of machine reading comprehension models. The dataset often requires reasoning over the quantitative content found in abstracts to answer the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e568983",
   "metadata": {},
   "source": [
    "## Preliminary initialization\n",
    "\n",
    "qui inseriamo tutte ciò che bisogna scaricare per far runnare il notebook così non ci sono problemi\n",
    "\n",
    "Eventualmente inseriamo il link della repo se ci sono file da scaricare\n",
    "\n",
    "<mark style=\"background-color: white; color: black;\">\n",
    "pip install datasets  <br>\n",
    "pip install pandas pyarrow\n",
    "</mark>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51674962",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e7d32af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\213053\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c77415",
   "metadata": {},
   "source": [
    "### Loading the dataset: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e301485",
   "metadata": {},
   "source": [
    "The object dataset is a DatasetDict, which contains different splits like \"train\", \"validation\", and \"test\" if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "251ebb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the labeled, unlabeled, and artificial subsets of PubMedQA\n",
    "# The dataset is split into three subsets:\n",
    "\n",
    "dataset_labeled = load_dataset(\"qiaojin/PubMedQA\", 'pqa_labeled')               \n",
    "dataset_unlabeled = load_dataset(\"qiaojin/PubMedQA\", 'pqa_unlabeled')    \n",
    "dataset_artificial = load_dataset('qiaojin/PubMedQA', 'pqa_artificial')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61b507b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled dataset: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['pubid', 'question', 'context', 'long_answer', 'final_decision'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n",
      "Artificial dataset: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['pubid', 'question', 'context', 'long_answer', 'final_decision'],\n",
      "        num_rows: 211269\n",
      "    })\n",
      "})\n",
      "Unlabeled dataset: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['pubid', 'question', 'context', 'long_answer'],\n",
      "        num_rows: 61249\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(\"Labeled dataset:\", dataset_labeled)\n",
    "print(\"Artificial dataset:\", dataset_artificial)\n",
    "print(\"Unlabeled dataset:\", dataset_unlabeled)\n",
    "\n",
    "# Notice that the feature final_decision is missing in the Unlabeled dataset, reflecting the fact \n",
    "# that these examples do not have a definitive yes/no/maybe label."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
