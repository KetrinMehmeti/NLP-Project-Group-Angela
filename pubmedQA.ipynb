{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e77cb13d",
   "metadata": {},
   "source": [
    "# Natural Language Processing Project\n",
    "## NLP Course @ Politecnico di Milano 2024/2025 - Prof. Mark Carman\n",
    "### Topic 8: Medical Question Answering\n",
    "Dataset: \n",
    "* PubMedQA [link](https://huggingface.co/datasets/qiaojin/PubMedQA )\n",
    "\n",
    "Reference paper:\n",
    "* PubMedQA: A Dataset for Biomedical Research Question Answering [Link](https://arxiv.org/pdf/1909.06146)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98da54ef",
   "metadata": {},
   "source": [
    "## Group members:\n",
    "\n",
    "* Ketrin Mehmeti\n",
    "* Giulia Ghiazza\n",
    "* Leonardo Giorgio Franco\n",
    "* Edoardo Franco Mattei\n",
    "* Alessandro Epifania"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3ac59c",
   "metadata": {},
   "source": [
    "## Introduction\n",
    " \n",
    "The PubMedQA dataset is an innovative resource for question answering (QA) in the biomedical field, created from abstracts of scientific articles available on PubMed. The main purpose of PubMedQA is to assess the reasoning and inference abilities of intelligent systems on natural language, particularly within the context of biomedical research texts, which often require the processing of quantitative content.\n",
    "\n",
    "A typical instance in PubMedQA consists of the following components:\n",
    "\n",
    "* A question, which can either be the original title of a research paper or derived from it. For example: \"Do preoperative statins reduce atrial fibrillation after coronary artery bypass grafting?\"\n",
    "\n",
    "* A context, which is the abstract corresponding to the question, excluding its conclusion.\n",
    "\n",
    "* A long answer, represented by the conclusion of the abstract, which is expected to answer the research question.\n",
    "\n",
    "* A short answer in the form of \"yes,\" \"no,\" or \"maybe,\" summarizing the conclusion. In the provided example, the long answer is: \"(Conclusion) Our study indicated that preoperative statin therapy seems to reduce AF development after CABG,\" while the short answer is \"yes.\"\n",
    "\n",
    "The PubMedQA dataset is divided into three subsets:\n",
    "\n",
    "* PQA-L (Labeled): Contains 1k manually annotated instances with yes/no/maybe answers. These annotations were made in two modes: \"reasoning-free,\" where the annotator had access to the long answer, and \"reasoning-required,\" where the annotator could only rely on the context.\n",
    "\n",
    "* PQA-U (Unlabeled): Consists of 61.2k unlabeled instances, made up of PubMed articles with question-form titles and structured abstracts.\n",
    "\n",
    "* PQA-A (Artificial): Includes 211.3k artificially generated instances, where article titles in statement form are converted into questions, and yes/no answers are automatically assigned based on the presence or absence of negations in the original title.\n",
    "\n",
    "A key feature of PubMedQA is that the contexts are generated to directly answer the questions, with both components written by the same authors, ensuring a strong relationship between the question and context. This makes PubMedQA an ideal benchmark for testing the scientific reasoning capabilities of machine reading comprehension models. The dataset often requires reasoning over the quantitative content found in abstracts to answer the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e568983",
   "metadata": {},
   "source": [
    "## Preliminary initialization\n",
    "\n",
    "qui inseriamo tutte ciò che bisogna scaricare per far runnare il notebook così non ci sono problemi\n",
    "\n",
    "Eventualmente inseriamo il link della repo se ci sono file da scaricare\n",
    "\n",
    "<mark style=\"background-color: white; color: black;\">\n",
    "pip install datasets  <br>\n",
    "pip install pandas pyarrow <br>\n",
    "pip install transformers\n",
    "</mark>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51674962",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2e7d32af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c77415",
   "metadata": {},
   "source": [
    "### Loading the dataset: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e301485",
   "metadata": {},
   "source": [
    "The object dataset is a DatasetDict, which contains different splits like \"train\", \"validation\", and \"test\" if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "251ebb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the labeled, unlabeled, and artificial subsets of PubMedQA\n",
    "# The dataset is split into three subsets:\n",
    "\n",
    "dataset_labeled = load_dataset(\"qiaojin/PubMedQA\", 'pqa_labeled')               \n",
    "dataset_unlabeled = load_dataset(\"qiaojin/PubMedQA\", 'pqa_unlabeled')    \n",
    "dataset_artificial = load_dataset('qiaojin/PubMedQA', 'pqa_artificial')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61b507b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled dataset: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['pubid', 'question', 'context', 'long_answer', 'final_decision'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n",
      "Artificial dataset: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['pubid', 'question', 'context', 'long_answer', 'final_decision'],\n",
      "        num_rows: 211269\n",
      "    })\n",
      "})\n",
      "Unlabeled dataset: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['pubid', 'question', 'context', 'long_answer'],\n",
      "        num_rows: 61249\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(\"Labeled dataset:\", dataset_labeled)\n",
    "print(\"Artificial dataset:\", dataset_artificial)\n",
    "print(\"Unlabeled dataset:\", dataset_unlabeled)\n",
    "\n",
    "# Notice that the feature final_decision is missing in the Unlabeled dataset, reflecting the fact \n",
    "# that these examples do not have a definitive yes/no/maybe label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ce57c28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train'])\n",
      "dict_keys(['train'])\n",
      "dict_keys(['train'])\n"
     ]
    }
   ],
   "source": [
    "print(dataset_labeled.keys())\n",
    "print(dataset_artificial.keys())\n",
    "print(dataset_unlabeled.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a20bb1",
   "metadata": {},
   "source": [
    "## Preliminary analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd777ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "Elemento non stringa in posizione 0: {'contexts': ['Programmed cell death (PCD) is the regulated death of cells within an organism. The lace plant (Aponogeton madagascariensis) produces perforations in its leaves through PCD. The leaves of the plant consist of a latticework of longitudinal and transverse veins enclosing areoles. PCD occurs in the cells at the center of these areoles and progresses outwards, stopping approximately five cells from the vasculature. The role of mitochondria during PCD has been recognized in animals; however, it has been less studied during PCD in plants.', 'The following paper elucidates the role of mitochondrial dynamics during developmentally regulated PCD in vivo in A. madagascariensis. A single areole within a window stage leaf (PCD is occurring) was divided into three areas based on the progression of PCD; cells that will not undergo PCD (NPCD), cells in early stages of PCD (EPCD), and cells in late stages of PCD (LPCD). Window stage leaves were stained with the mitochondrial dye MitoTracker Red CMXRos and examined. Mitochondrial dynamics were delineated into four categories (M1-M4) based on characteristics including distribution, motility, and membrane potential (ΔΨm). A TUNEL assay showed fragmented nDNA in a gradient over these mitochondrial stages. Chloroplasts and transvacuolar strands were also examined using live cell imaging. The possible importance of mitochondrial permeability transition pore (PTP) formation during PCD was indirectly examined via in vivo cyclosporine A (CsA) treatment. This treatment resulted in lace plant leaves with a significantly lower number of perforations compared to controls, and that displayed mitochondrial dynamics similar to that of non-PCD cells.'], 'labels': ['BACKGROUND', 'RESULTS'], 'meshes': ['Alismataceae', 'Apoptosis', 'Cell Differentiation', 'Mitochondria', 'Plant Leaves'], 'reasoning_required_pred': ['y', 'e', 's'], 'reasoning_free_pred': ['y', 'e', 's']} (tipo <class 'dict'>)\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# check per vedere se sono tutte stringhe se no tokenizer non funziona\n",
    "\n",
    "print(type( dataset_labeled['train']['question'] ))\n",
    "\n",
    "for i, x in enumerate(dataset_labeled['train']['question']):\n",
    "    if not isinstance(x, str):\n",
    "        print(f\"Elemento non stringa in posizione {i}: {x} (tipo {type(x)})\")\n",
    "        break\n",
    "\n",
    "print(type( dataset_labeled['train']['context'] ))\n",
    "\n",
    "for i, x in enumerate(dataset_labeled['train']['context']):\n",
    "    if not isinstance(x, str):\n",
    "        print(f\"Elemento non stringa in posizione {i}: {x} (tipo {type(x)})\")\n",
    "        break\n",
    "\n",
    "print(type( dataset_labeled['train']['long_answer'] ))\n",
    "\n",
    "for i, x in enumerate(dataset_labeled['train']['long_answer']):\n",
    "    if not isinstance(x, str):\n",
    "        print(f\"Elemento non stringa in posizione {i}: {x} (tipo {type(x)})\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9d8549",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m tokenized_questions_unlabeled = tokenizer(questions_unlabeled, add_special_tokens=\u001b[38;5;28;01mFalse\u001b[39;00m, padding=\u001b[38;5;28;01mFalse\u001b[39;00m, truncation=\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     26\u001b[39m tokenized_questions_artificial = tokenizer(questions_artificial, add_special_tokens=\u001b[38;5;28;01mFalse\u001b[39;00m, padding=\u001b[38;5;28;01mFalse\u001b[39;00m, truncation=\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m tokenized_contexts_labeled = \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext_labeled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     29\u001b[39m tokenized_contexts_unlabeled = tokenizer(context_unlabeled, add_special_tokens=\u001b[38;5;28;01mFalse\u001b[39;00m, padding=\u001b[38;5;28;01mFalse\u001b[39;00m, truncation=\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     30\u001b[39m tokenized_contexts_artificial = tokenizer(context_artificial, add_special_tokens=\u001b[38;5;28;01mFalse\u001b[39;00m, padding=\u001b[38;5;28;01mFalse\u001b[39;00m, truncation=\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:2887\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.__call__\u001b[39m\u001b[34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m   2885\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._in_target_context_manager:\n\u001b[32m   2886\u001b[39m         \u001b[38;5;28mself\u001b[39m._switch_to_input_mode()\n\u001b[32m-> \u001b[39m\u001b[32m2887\u001b[39m     encodings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2888\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2889\u001b[39m     \u001b[38;5;28mself\u001b[39m._switch_to_target_mode()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:2975\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._call_one\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[39m\n\u001b[32m   2970\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2971\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not match batch length of `text_pair`:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2972\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2973\u001b[39m         )\n\u001b[32m   2974\u001b[39m     batch_text_or_text_pairs = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[32m-> \u001b[39m\u001b[32m2975\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2976\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2977\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2978\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2979\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2980\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2981\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2982\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2983\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2984\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2985\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2986\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2987\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2988\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2989\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2990\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2991\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2992\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2993\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2994\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2995\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2996\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2997\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.encode_plus(\n\u001b[32m   2998\u001b[39m         text=text,\n\u001b[32m   2999\u001b[39m         text_pair=text_pair,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3017\u001b[39m         **kwargs,\n\u001b[32m   3018\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:3177\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.batch_encode_plus\u001b[39m\u001b[34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[39m\n\u001b[32m   3167\u001b[39m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[32m   3168\u001b[39m padding_strategy, truncation_strategy, max_length, kwargs = \u001b[38;5;28mself\u001b[39m._get_padding_truncation_strategies(\n\u001b[32m   3169\u001b[39m     padding=padding,\n\u001b[32m   3170\u001b[39m     truncation=truncation,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3174\u001b[39m     **kwargs,\n\u001b[32m   3175\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3178\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3179\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3180\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3181\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3182\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3183\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3184\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3185\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3186\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3187\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3188\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3189\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3190\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3191\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3192\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3193\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3194\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3195\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3196\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3197\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\tokenization_utils_fast.py:539\u001b[39m, in \u001b[36mPreTrainedTokenizerFast._batch_encode_plus\u001b[39m\u001b[34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[39m\n\u001b[32m    536\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tokenizer.encode_special_tokens != split_special_tokens:\n\u001b[32m    537\u001b[39m     \u001b[38;5;28mself\u001b[39m._tokenizer.encode_special_tokens = split_special_tokens\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m encodings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    542\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_pretokenized\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    543\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[32m    546\u001b[39m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[32m    547\u001b[39m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[32m    548\u001b[39m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[32m    549\u001b[39m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[32m    550\u001b[39m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[32m    551\u001b[39m tokens_and_encodings = [\n\u001b[32m    552\u001b[39m     \u001b[38;5;28mself\u001b[39m._convert_encoding(\n\u001b[32m    553\u001b[39m         encoding=encoding,\n\u001b[32m   (...)\u001b[39m\u001b[32m    562\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[32m    563\u001b[39m ]\n",
      "\u001b[31mTypeError\u001b[39m: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Extract the questions, contexts, and long answers from the datasets\n",
    "questions_labeled = dataset_labeled['train']['question']\n",
    "questions_unlabeled = dataset_unlabeled['train']['question']\n",
    "questions_artificial = dataset_artificial['train']['question']\n",
    "\n",
    "# PROBLEMA DA RISOLVERE: il contesto è una lista di liste, non una lista di stringhe\n",
    "# QUINDI NON FUNZIONA IL TOKENIZER!!!!!!!!!!!!!!!!!!\n",
    "context_labeled = [x[\"contexts\"] for x in dataset_labeled['train']['context']]\n",
    "context_unlabeled = [x[\"contexts\"] for x in dataset_unlabeled['train']['context']]\n",
    "context_artificial = [x[\"contexts\"] for x in dataset_artificial['train']['context']]\n",
    "\n",
    "#context_labeled = [\" \".join(x[\"contexts\"]) for x in dataset_labeled['train']['context']]\n",
    "#context_unlabeled = [\" \".join(x[\"contexts\"]) for x in dataset_unlabeled['train']['context']]\n",
    "#context_artificial = [\" \".join(x[\"contexts\"]) for x in dataset_artificial['train']['context']]\n",
    "\n",
    "long_answers_labeled = dataset_labeled['train']['long_answer']\n",
    "long_answers_unlabeled = dataset_unlabeled['train']['long_answer']\n",
    "long_answers_artificial = dataset_artificial['train']['long_answer']\n",
    "\n",
    "# Tokenize the questions, contexts, and long answers\n",
    "# Note: We are not adding special tokens, padding, or truncating the sequences here because we want to keep the original lengths.\n",
    "tokenized_questions_labeled = tokenizer(questions_labeled, add_special_tokens=False, padding=False, truncation=False)[\"input_ids\"]\n",
    "tokenized_questions_unlabeled = tokenizer(questions_unlabeled, add_special_tokens=False, padding=False, truncation=False)[\"input_ids\"]\n",
    "tokenized_questions_artificial = tokenizer(questions_artificial, add_special_tokens=False, padding=False, truncation=False)[\"input_ids\"]\n",
    "\n",
    "tokenized_contexts_labeled = tokenizer(context_labeled, add_special_tokens=False, padding=False, truncation=False)[\"input_ids\"]\n",
    "tokenized_contexts_unlabeled = tokenizer(context_unlabeled, add_special_tokens=False, padding=False, truncation=False)[\"input_ids\"]\n",
    "tokenized_contexts_artificial = tokenizer(context_artificial, add_special_tokens=False, padding=False, truncation=False)[\"input_ids\"]\n",
    "\n",
    "tokenized_long_answers_labeled = tokenizer(long_answers_labeled, add_special_tokens=False, padding=False, truncation=False)[\"input_ids\"]\n",
    "tokenized_long_answers_unlabeled = tokenizer(long_answers_unlabeled, add_special_tokens=False, padding=False, truncation=False)[\"input_ids\"]\n",
    "tokenized_long_answers_artificial = tokenizer(long_answers_artificial, add_special_tokens=False, padding=False, truncation=False)[\"input_ids\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d87a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train'])\n",
      "dict_keys(['train'])\n",
      "dict_keys(['train'])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cda49a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
